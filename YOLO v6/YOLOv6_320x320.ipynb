{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD9gUQpaBxNa"
      },
      "source": [
        "# How to Train YOLOv6 on a Custom Dataset\n",
        "\n",
        "This tutorial is based on the [YOLOv6 repository](https://github.com/meituan/YOLOv6) (officially \"MT-YOLOv6\") by Meituan. This notebook shows training on **your own custom objects**. Many thanks to Meituan for putting this repository together.\n",
        "\n",
        "\n",
        "### **Accompanying Blog Post**\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on [how to train YOLOv6](https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset/), concurrently.\n",
        "\n",
        "### **Steps Covered in this Tutorial**\n",
        "\n",
        "In this tutorial, we will walk through the steps required to train YOLOv6 on your custom objects. We use a [public chess pieces detection dataset](https://universe.roboflow.com/joseph-nelson/chess-pieces-new), which is open source and free to use. (There are 90,000+ more computer vision datasets [here](https://universe.roboflow.com/).)\n",
        "\n",
        "To train our detector we take the following steps:\n",
        "\n",
        "* Prepare our dataset in MT-YOLOv6 format\n",
        "* Install MT-YOLOv6 dependencies\n",
        "* Load custom dataset\n",
        "* Run MT-YOLOv6 training\n",
        "* Evaluate MT-YOLOv6 performance\n",
        "* Visualize MT-YOLOv5 training data\n",
        "* Run MT-YOLOv6 inference on test images\n",
        "* OPTIONAL: Deployment\n",
        "* OPTIONAL: Active Learning\n",
        "\n",
        "\n",
        "### Easier Dataset Prep\n",
        "\n",
        "This dataset was prepared using [Roboflow](https://roboflow.com), a set of tools developers use to build better computer vision models quickly and accurately. 100k+ developers use roboflow for (automatic) annotation, converting dataset formats (like to YOLOv6), training, deploying, and improving their datasets/models.\n",
        "\n",
        "Members of the Roboflow community also share 60M+ images, 90,000+ projects, and 7,000+ pretrained models: https://universe.roboflow.com\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "#Install Dependencies\n",
        "\n",
        "_(Remember to choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD-uPyQ_2jiN",
        "outputId": "bf065be9-d54e-49b9-e7a7-47a415246d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'YOLOv6'...\n",
            "remote: Enumerating objects: 2716, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 2716 (delta 1), reused 0 (delta 0), pack-reused 2709\u001b[K\n",
            "Receiving objects: 100% (2716/2716), 34.55 MiB | 19.27 MiB/s, done.\n",
            "Resolving deltas: 100% (1525/1525), done.\n",
            "/content/YOLOv6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (0.14.0+cu116)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (4.6.0.66)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (6.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (1.7.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (4.64.1)\n",
            "Collecting addict>=2.4.0\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: tensorboard>=2.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (2.9.1)\n",
            "Requirement already satisfied: pycocotools>=2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (2.0.6)\n",
            "Collecting onnx>=1.10.0\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx-simplifier>=0.3.6\n",
            "  Downloading onnx_simplifier-0.4.13-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.0->-r requirements.txt (line 4)) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.0->-r requirements.txt (line 5)) (2.25.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (3.19.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (1.51.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.7.0->-r requirements.txt (line 12)) (2.15.0)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from pycocotools>=2.0->-r requirements.txt (line 13)) (3.2.2)\n",
            "Collecting onnx>=1.10.0\n",
            "  Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich\n",
            "  Downloading rich-13.3.1-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.7.0->-r requirements.txt (line 12)) (5.2.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.7.0->-r requirements.txt (line 12)) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.7.0->-r requirements.txt (line 12)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.7.0->-r requirements.txt (line 12)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.7.0->-r requirements.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.7.0->-r requirements.txt (line 12)) (6.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0->-r requirements.txt (line 13)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0->-r requirements.txt (line 13)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0->-r requirements.txt (line 13)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0->-r requirements.txt (line 13)) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.0->-r requirements.txt (line 5)) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.0->-r requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.0->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.0->-r requirements.txt (line 5)) (4.0.0)\n",
            "Collecting markdown-it-py<3.0.0,>=2.1.0\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.14.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.7.0->-r requirements.txt (line 12)) (3.11.0)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.7.0->-r requirements.txt (line 12)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.7.0->-r requirements.txt (line 12)) (3.2.2)\n",
            "Installing collected packages: addict, pygments, onnx, mdurl, thop, markdown-it-py, rich, onnx-simplifier\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed addict-2.4.0 markdown-it-py-2.1.0 mdurl-0.1.2 onnx-1.12.0 onnx-simplifier-0.4.13 pygments-2.14.0 rich-13.3.1 thop-0.1.1.post2209072238\n"
          ]
        }
      ],
      "source": [
        "# Download MT-YOLOv6 repository and install requirements\n",
        "!git clone https://github.com/meituan/YOLOv6\n",
        "%cd YOLOv6\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyD5KHSUr5ow"
      },
      "source": [
        "# Prepare Custom Dataset\n",
        "\n",
        "We need our dataset in the YOLOv6 format, which requires YOLO TXT annotations, organized directories, and a specific `.yaml` config file. \n",
        "\n",
        "- If you're following the custom chess dataset example, use the YOLOv6 format chess dataset export [here](https://universe.roboflow.com/joseph-nelson/chess-pieces-new/23/export).\n",
        "\n",
        "- If you're preparing your own data, use the guide for creating, formatting, and exporting your custom dataset [here](https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjH64g7CyAuG"
      },
      "source": [
        "**The YOLOv6 format is as follows:**\n",
        "\n",
        "[YOLO TXT format](https://roboflow.com/formats/yolo-darknet-txt)\n",
        "```\n",
        "# class_id center_x center_y bbox_width bbox_height\n",
        "1 0.408 0.30266666666666664 0.104 0.15733333333333333\n",
        "```\n",
        "\n",
        "Dataset directory format\n",
        "```\n",
        "# image directory\n",
        "path/to/data/images/train/im0.jpg\n",
        "path/to/data/images/val/im1.jpg\n",
        "path/to/data/images/test/im2.jpg\n",
        "\n",
        "# label directory\n",
        "path/to/data/labels/train/im0.txt\n",
        "path/to/data/labels/val/im1.txt\n",
        "path/to/data/labels/test/im2.txt\n",
        "```\n",
        "\n",
        "`YAML` format\n",
        "\n",
        "```\n",
        "train: ./images/train\n",
        "val: ./images/valid\n",
        "test: ./images/test\n",
        "\n",
        "nc: 12\n",
        "names: ['black-bishop', 'black-king', 'black-knight', 'black-pawn', 'black-queen', 'black-rook', 'white-bishop', 'white-king', 'white-knight', 'white-pawn', 'white-queen', 'white-rook']\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTUMvCXgyV2-"
      },
      "source": [
        "For a step-by-step on getting your data into this correct format, follow the blog post here: https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtJ24mPlyF-S"
      },
      "source": [
        "# Download Correctly Formatted Custom Data\n",
        "\n",
        "Next, we'll download our dataset in the right format. Use the `meituan/YOLOv6 PyTorch` export. Note that the Meituan implementation requires YOLO TXT annotations, a custom YAML file, and organized directories. The roboflow export writes this for us. (See [this guide](https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset/) for more details.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcv8LVG54KOx"
      },
      "source": [
        "![YOLOv6 export](https://i.imgur.com/W9IIZxr.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ovKgrVN8ygdW",
        "outputId": "b22e629d-27ca-48ce-d178-35ab994fecc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-0.2.29-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2022.12.7)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.10)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from roboflow) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.21.6)\n",
            "Collecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting requests-toolbelt\n",
            "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.64.1)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.21.1-py3-none-any.whl (19 kB)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urllib3>=1.26.6\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.6.0.66)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.25.1)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9674 sha256=9291e8486817e27c24f166a6eabb2604e5fc2af229f356c5de2a20840d3f724c\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, urllib3, python-dotenv, pyparsing, cycler, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "Successfully installed cycler-0.10.0 pyparsing-2.4.7 python-dotenv-0.21.1 requests-toolbelt-0.10.1 roboflow-0.2.29 urllib3-1.26.14 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in COCO-dataset2-30 to mt-yolov6: 100% [12292585 / 12292585] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to COCO-dataset2-30 in mt-yolov6:: 100%|██████████| 1355/1355 [00:00<00:00, 2151.88it/s]\n"
          ]
        }
      ],
      "source": [
        "# REPLACE with your custom code snippet generated above to use your data\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"jv4SX75HBATaTyvFy6c6\")\n",
        "project = rf.workspace(\"university-malaya\").project(\"coco-dataset2\")\n",
        "dataset = project.version(30).download(\"mt-yolov6\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeFisESy5s8m"
      },
      "source": [
        "# Custom Training Details\n",
        "\n",
        "There are a number of ways to fine tune training of YOLOv6, like custom configuration files for fine tuning, multi GPU support, and passing custom training arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmyI3ILXBlbE"
      },
      "source": [
        "\n",
        "### Multi GPU Support\n",
        "\n",
        "YOLOv6 supports single and multi GPU training.\n",
        "\n",
        "Single GPU:\n",
        "```\n",
        "python tools/train.py --batch 256 --conf configs/yolov6s_finetune.py --data data/data.yaml --device 0\n",
        "```\n",
        "\n",
        "Multi GPU:\n",
        "```\n",
        "python -m torch.distributed.launch --nproc_per_node 4 tools/train.py --batch 256 --conf configs/yolov6s_finetune.py --data data/data.yaml --device 0,1,2,3\n",
        "```\n",
        "\n",
        "### Custom Configuration\n",
        "\n",
        "YOLOv6 also supports creating a custom configuration file for training YOLOv6-n, YOLOv6-tiny, and YOLOv6s. (YOLOv6 m/l/x are coming soon.) You can also start training with a configuration that uses finetuning (e.g. `yolov6s_finetune.py` or starting from scratch (`yolov6s.py`). Finetuning will train faster though may not be as effective on unique datasets.\n",
        "\n",
        "### Default Arguments\n",
        "- --data-path, default='./data/coco.yaml', type=str, help='path of dataset')\n",
        "- --conf-file, default='./configs/yolov6s.py', type=str, help='experiments description file')\n",
        "- --img-size, type=int, default=640, help='train, val image size (pixels)')\n",
        "- --batch-size, default=32, type=int, help='total batch size for all GPUs')\n",
        "- --epochs, default=400, type=int, help='number of total epochs to run')\n",
        "- --workers, default=8, type=int, help='number of data loading workers (default: 8)')\n",
        "- --device, default='0', type=str, help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "- --eval-interval, type=int, default=20, help='evaluate at every interval epochs')\n",
        "- --eval-final-only, action='store_true', help='only evaluate at the final epoch')\n",
        "- --heavy-eval-range, default=50,help='evaluating every epoch for last such epochs (can be jointly used with --eval-interval)')\n",
        "- --check-images, action='store_true', help='check images when initializing datasets')\n",
        "- --check-labels, action='store_true', help='check label files when initializing datasets')\n",
        "- --output-dir, default='./runs/train', type=str, help='path to save outputs')\n",
        "- --name, default='exp', type=str, help='experiment name, saved to output_dir/name')\n",
        "- --dist_url, type=str, default=\"default url: tcp://127.0.0.1:8888\")\n",
        "- --gpu_count, type=int, default=0)\n",
        "- --local_rank, type=int, default=-1, help='DDP parameter')\n",
        "- --resume, type=str, default=None, help='resume the corresponding ckpt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHfT9gEiBsBd"
      },
      "source": [
        "# Begin Custom Training\n",
        "\n",
        "We're ready to start custom training.\n",
        "\n",
        "NOTE: We will modify two of the YOLOv6 training defaults in our custom training example: `epochs` and `image-size`. We will adjust from 400 to 100 epochs in our example for speed. Similarly, we will adjust image size from 600x600 to 416x416, which is the default size for other YOLO models (and makes comparisons easier as well as training slightly faster).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iqOPKjr22mL",
        "outputId": "d1056463-c488-4fa7-bdd7-c48148ef5631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"tools/train.py\", line 128, in <module>\n",
            "    main(args)\n",
            "  File \"tools/train.py\", line 101, in main\n",
            "    cfg, device, args = check_and_init(args)\n",
            "  File \"tools/train.py\", line 87, in check_and_init\n",
            "    device = select_device(args.device)\n",
            "  File \"/content/YOLOv6/yolov6/utils/envs.py\", line 32, in select_device\n",
            "    assert torch.cuda.is_available()\n",
            "AssertionError\n"
          ]
        }
      ],
      "source": [
        "# run this cell to begin training\n",
        "!python tools/train.py --batch 32 --conf configs/yolov6s.py --epochs 2 --img-size 320 --data {dataset.location}/data.yaml --device 0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W0MpUaTCJro"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "We can evaluate the performance of our custom training using the provided evalution script.\n",
        "\n",
        "Note we can adjust the below custom arguments.\n",
        "\n",
        "- --data, type=str, default='./data/coco.yaml', help='dataset.yaml path')\n",
        "- -weights, type=str, default='./weights/yolov6s.pt', help='model.pt path(s)')\n",
        "- --batch-size, type=int, default=32, help='batch size')\n",
        "- --img-size, type=int, default=640, help='inference size (pixels)')\n",
        "- --conf-thres, type=float, default=0.001, help='confidence threshold')\n",
        "- --iou-thres, type=float, default=0.65, help='NMS IoU threshold')\n",
        "- --task, default='val', help='val, or speed')\n",
        "- --device, default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "- --half, default=False, action='store_true', help='whether to use fp16 infer')\n",
        "- --save_dir, type=str, default='runs/val/', help='evaluation save dir')\n",
        "- -name, type=str, default='exp', help='save evaluation results to save_dir/name')\n",
        "\n",
        "Similar to training, we will pass 416x416 images for evaluation as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4cfnLtTCIce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b2516f-c4b9-493b-f11c-d268cebb0bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=32, conf_thres=0.03, config_file='', data='/content/YOLOv6/COCO-dataset2-30/data.yaml', device='0', do_coco_metric=True, do_pr_metric=False, eval_config_file='./configs/experiment/eval_640_repro.py', force_no_pad=False, half=False, img_size=320, iou_thres=0.65, letterbox_return_int=False, name='exp', not_infer_on_rect=False, plot_confusion_matrix=False, plot_curve=True, reproduce_640_eval=False, save_dir='runs/val/', scale_exact=False, task='val', test_load_size=640, verbose=False, weights='runs/train/exp/weights/best_ckpt.pt')\n",
            "Loading checkpoint from runs/train/exp/weights/best_ckpt.pt\n",
            "\n",
            "Fusing model...\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Switch model to deploy modality.\n",
            "Model Summary: Params: 18.50M, Gflops: 11.29\n",
            "Val: Checking formats of labels with 2 process(es): \n",
            "20 label(s) found, 0 label(s) missing, 0 label(s) empty, 0 invalid label files: 100% 20/20 [00:00<00:00, 10501.51it/s]\n",
            "Convert to COCO format\n",
            "100% 20/20 [00:00<00:00, 71881.82it/s]\n",
            "Convert to COCO format finished. Resutls saved in COCO-dataset2-30/annotations/instances_valid.json\n",
            "Val: Final numbers of valid images: 20/ labels: 20. \n",
            "0.1s for dataset initialization.\n",
            "Inferencing model in val datasets.: 100% 1/1 [00:01<00:00,  1.07s/it]\n",
            "\n",
            "Evaluating speed.\n",
            "Average pre-process time: 0.06 ms\n",
            "Average inference time: 6.51 ms\n",
            "Average NMS time: 15.56 ms\n",
            "\n",
            "Evaluating mAP by pycocotools.\n",
            "Saving runs/val/exp/predictions.json...\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.53s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.072\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.016\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.109\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.016\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.059\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.062\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.157\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.300\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.134\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550\n",
            "Results saved to runs/val/exp\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation\n",
        "!python tools/eval.py --data {dataset.location}/data.yaml --img-size 320 --weights runs/train/exp/weights/best_ckpt.pt --device 0 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-p3rhvpCRTZ"
      },
      "source": [
        "# Inference\n",
        "\n",
        "We can run inference on images of our custom trained model using the provided inference utility.\n",
        "\n",
        "There are a number of arguments we can adjust:\n",
        "\n",
        "- --weights, type=str, default='weights/yolov6s.pt', help='model path(s) for inference.')\n",
        "- --source, type=str, default='data/images', help='the source path, e.g. image-file/dir.')\n",
        "- --yaml, type=str, default='data/coco.yaml', help='data yaml file.')\n",
        "- --img-size, type=int, default=640, help='the image-size(h,w) in inference size.')\n",
        "- --conf-thres, type=float, default=0.25, help='confidence threshold for inference.')\n",
        "- --iou-thres, type=float, default=0.45, help='NMS IoU threshold for inference.')\n",
        "- --max-det, type=int, default=1000, help='maximal inferences per image.')\n",
        "- --device, default='0', help='device to run our model i.e. 0 or 0,1,2,3 or cpu.')\n",
        "- --save-txt, action='store_true', help='save results to *.txt.')\n",
        "- --save-img, action='store_false', help='save visuallized inference results.')\n",
        "- --classes, nargs='+', type=int, help='filter by classes, e.g. --classes 0, or --classes 0 2 3.')\n",
        "- --agnostic-nms, action='store_true', help='class-agnostic NMS.')\n",
        "- --project, default='runs/inference', help='save inference results to project/name.')\n",
        "- -name, default='exp', help='save inference results to project/name.')\n",
        "- --hide-labels, default=False, action='store_true', help='hide labels.')\n",
        "- --hide-conf, default=False, action='store_true', help='hide confidences.')\n",
        "- --half, action='store_true', help='whether to use FP16 half-precision inference.')\n",
        "\n",
        "We need to pass our custom `.yaml` file so that our label names are correct. We will also pass our `/test` directory to run inference on all images in our test split. In addition, similar to training, we will pass 416x416 images for inference as an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXgmugu0BE7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7545a698-16f3-4ad4-f481-aafde025f840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(agnostic_nms=False, classes=None, conf_thres=0.4, device='0', half=False, hide_conf=False, hide_labels=False, img_size=[320], iou_thres=0.45, max_det=1000, name='exp', not_save_img=False, project='runs/inference', save_dir=None, save_txt=False, source='/content/YOLOv6/COCO-dataset2-30/images/test/', view_img=False, webcam=False, webcam_addr='0', weights='runs/train/exp/weights/best_ckpt.pt', yaml='/content/YOLOv6/COCO-dataset2-30/data.yaml')\n",
            "Loading checkpoint from runs/train/exp/weights/best_ckpt.pt\n",
            "\n",
            "Fusing model...\n",
            "Switch model to deploy modality.\n",
            "Traceback (most recent call last):\n",
            "  File \"tools/infer.py\", line 120, in <module>\n",
            "    main(args)\n",
            "  File \"tools/infer.py\", line 115, in main\n",
            "    run(**vars(args))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"tools/infer.py\", line 107, in run\n",
            "    inferer = Inferer(source, webcam, webcam_addr, weights, device, yaml, img_size, half)\n",
            "  File \"/content/YOLOv6/yolov6/core/inferer.py\", line 50, in __init__\n",
            "    self.model(torch.zeros(1, 3, *self.img_size).to(self.device).type_as(next(self.model.model.parameters())))  # warmup\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/YOLOv6/yolov6/layers/common.py\", line 410, in forward\n",
            "    y, _ = self.model(im)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/YOLOv6/yolov6/models/yolo.py\", line 34, in forward\n",
            "    x = self.backbone(x)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/YOLOv6/yolov6/models/efficientrep.py\", line 106, in forward\n",
            "    x = self.stem(x)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/YOLOv6/yolov6/layers/common.py\", line 259, in forward\n",
            "    return self.nonlinearity(self.se(self.rbr_reparam(inputs)))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "RuntimeError: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 1, 3, 320] to have 3 channels, but got 1 channels instead\n"
          ]
        }
      ],
      "source": [
        "# infer on all images in our /test directory\n",
        "!python tools/infer.py --yaml {dataset.location}/data.yaml --img-size 320 --weights runs/train/exp/weights/best_ckpt.pt --source {dataset.location}/images/test/ --device 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AGhNOSSHY4_"
      },
      "outputs": [],
      "source": [
        "# display test inference result images\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "i = 0\n",
        "limit = 3 # max images to print\n",
        "for imageName in glob.glob('./runs/inference/exp/*.jpg'): #assuming JPG\n",
        "    if i < limit:\n",
        "      display(Image(filename=imageName))\n",
        "      print(\"\\n\\n\")\n",
        "    i = i + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jn4kCtgKiGO"
      },
      "source": [
        "# OPTIONAL: Deployment\n",
        "\n",
        "There is a [utility included](https://github.com/meituan/YOLOv6/tree/main/deploy/ONNX) to export the model as ONNX format for deployment as well:\n",
        "```\n",
        "python deploy/ONNX/export_onnx.py --weights runs/train/exp/weights/best_ckpt.pt --device 0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f41PvE5gKhYw"
      },
      "source": [
        "# OPTIONAL: Active Learning Example\n",
        "\n",
        "Once our first training run is complete, we should use our model to help identify which images are most problematic in order to investigate, annotate, and improve our dataset (and, therefore, model).\n",
        "\n",
        "To do that, we can execute code that automatically uploads images back to our hosted dataset if the image is a specific class or below a given confidence threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcINqQS7Kt3-"
      },
      "outputs": [],
      "source": [
        "# # setup access to your workspace\n",
        "# rf = Roboflow(api_key=\"YOUR_API_KEY\")                               # used above to load data\n",
        "# inference_project =  rf.workspace().project(\"YOUR_PROJECT_NAME\")    # used above to load data\n",
        "# model = inference_project.version(1).model\n",
        "\n",
        "# upload_project = rf.workspace().project(\"YOUR_PROJECT_NAME\")\n",
        "\n",
        "# print(\"inference reference point: \", inference_project)\n",
        "# print(\"upload destination: \", upload_project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEl1NVE3LSD_"
      },
      "outputs": [],
      "source": [
        "# # example upload: if prediction is below a given confidence threshold, upload it \n",
        "\n",
        "# confidence_interval = [10,70]                                   # [lower_bound_percent, upper_bound_percent]\n",
        "\n",
        "# for prediction in predictions:                                  # predictions list to loop through\n",
        "#   if(prediction['confidence'] * 100 >= confidence_interval[0] and \n",
        "#           prediction['confidence'] * 100 <= confidence_interval[1]):\n",
        "        \n",
        "#           # upload on success!\n",
        "#           print(' >> image uploaded!')\n",
        "#           upload_project.upload(image, num_retry_uploads=3)     # upload image in question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVpCFeU-K4gb"
      },
      "source": [
        "# Congrats! Happy training.\n",
        "\n",
        "Hope you enjoyed this."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}